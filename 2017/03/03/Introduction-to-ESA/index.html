<!DOCTYPE html>
<html>
  <!-- Html Head Tag-->
  <head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="description" content="">
  <meta name="author" content="Yhuann">
  <!-- Open Graph Data -->
  <meta property="og:title" content="Introduction to ESA"/>
  <meta property="og:description" content="start from zero" />
  <meta property="og:site_name" content="May be a new world"/>
  <meta property="og:type" content="article" />
  <meta property="og:image" content="http://yoursite.comundefined"/>
  
    <link rel="alternate" href="/atom.xml" title="May be a new world" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  

  <!-- Site Title -->
  <title>May be a new world</title>

  <!-- Bootstrap CSS -->
  <link rel="stylesheet" href="/css/bootstrap.min.css">
  <!-- Custom CSS -->
  
  <link rel="stylesheet" href="/css/style.light.css">

  <!-- Google Analytics -->
  

</head>

  <body>
    <!-- Page Header -->


<header class="site-header header-background" style="background-image: url(/img/default-banner-dark.jpg)">
  <div class="container">
    <div class="row">
      <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
        <div class="page-title with-background-image">
          <p class="title">Introduction to ESA</p>
          <p class="subtitle"></p>
        </div>
        <div class="site-menu with-background-image">
          <ul>
            
              <li>
                <a href="/">
                  
                  Home
                  
                </a>
              </li>
            
              <li>
                <a href="/archives">
                  
                  Archives
                  
                </a>
              </li>
            
              <li>
                <a href="https://github.com/levblanc">
                  
                  Github
                  
                </a>
              </li>
            
              <li>
                <a href="mailto:levblanc@163.com">
                  
                  Email
                  
                </a>
              </li>
            
          </ul>
        </div>
      </div>
    </div>
  </div>
</header>


<article>
  <div class="container typo">
    <div class="row">
      <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
        <div class="post-info text-muted">
          
            <!-- Author -->
            <span class="author info">By Yhuann</span>
          
          <!-- Date -->
          <span class="date-time info">On
            <span class="date">2017-03-03</span>
            <span class="time">23:58:23</span>
          </span>
          
        </div>
        <!-- Tags -->
        
        <!-- Post Main Content -->
        <div class="post-content">
          <h1 id="Introduction-to-ESA"><a href="#Introduction-to-ESA" class="headerlink" title="Introduction to ESA"></a>Introduction to ESA</h1><p>@(ESA论文阅读笔记)[马克飞象|Markdown]</p>
<p><strong>ESA</strong>：Explicit Semantic Analysis， a novel method that represents the meaning of texts in high-dimensional space of concepts derived from Wikipedia.</p>
<ul>
<li><strong>显性表达文本</strong> ：以此作为基于维基百科的概念的权重向量；</li>
<li><strong>评估相关性</strong> ：用常规的度量比较相应的向量，如余弦；</li>
<li><strong>与现有技术相比</strong> ：ESA得到的结果相比于其他技术与人类的判断的相关性更高：对于单个词，从0.56到0.75；对于文本从0.6到0.75；</li>
<li><strong>更易理解</strong>：由于利用了自然概念，ESA模型更易被人理解。</li>
</ul>
<hr>
<p>[TOC]</p>
<hr>
<h2 id="Computing-Semantic-Relatedness-using-Wikipedia-based-Explicit-Semantic-Analysis"><a href="#Computing-Semantic-Relatedness-using-Wikipedia-based-Explicit-Semantic-Analysis" class="headerlink" title="Computing Semantic Relatedness using Wikipedia-based Explicit Semantic Analysis"></a>Computing Semantic Relatedness using Wikipedia-based Explicit Semantic Analysis</h2><p><a href="file:///D:/文档/literature/Computing%20Semantic%20Relatedness%20using%20Wikipedia-based%20Explicit%20Semantic%20Analysis.pdf" target="_blank" rel="external"><code>文献1</code></a></p>
<p>Author : <strong>Evgeniy Gabrilovich</strong> and <strong>Shaul Markovitch</strong><br>Department of Computer Science Technion—–Israel Institute of Technology, 32000 Haifa, Israel<br>IJCAI-07</p>
<hr>
<p>###理论介绍</p>
<p>####三个贡献：</p>
<ul>
<li>提出一种新的方法：用<strong>自然概念表达</strong>自然语言文本语义；</li>
<li>针对<strong>单词</strong>和<strong>文本</strong>，提出一个<strong>统一的计算相关性的方式</strong>；</li>
<li>用ESA模型计算的结果要优于现有的方法</li>
<li><strong>容易理解</strong></li>
</ul>
<p>####目的：<br><strong>Represent texts</strong> as a <strong>weighted mixture</strong> of a predetermined set of <strong>natural concepts</strong>, which are defined by humans themselves and can be easily explained.</p>
<p>####选择用Wikepedia的原因：</p>
<ul>
<li>发展稳定的基础上，词典深度、广度一直在增加</li>
<li>目前是网络上最大的知识库</li>
<li>准确性与大不列颠相似</li>
</ul>
<hr>
<p>####转化</p>
<ul>
<li>构建<strong>语义解释器（semantic interpreter）</strong>：将自然语言的片段映射到维基百科概念（Wikipedia concepts）的加权序列，并按照维基百科概念与输入文本的相关性对维基百科所有概念（concepts）进行排序——将文本的语义转化为文本与维基百科概念的亲密性（affinity）的表示。</li>
<li>计算语义相关性转化为两个向量的相似性比较。</li>
<li>维基百科概念（concepts）被表示为词向量，用TFIDF分配权重——词与概念（concepts）的关联强度。</li>
</ul>
<hr>
<p>####实现过程</p>
<ul>
<li>输入文本$T = {w_i }$，$<v_i>$表示TFIDF向量，是Wi的权重，$<k_j>$是Cj的倒排索引矩阵，量化了Wi与维基百科概念Cj ${c_j\in c_1,c_2……c_N}$（N为维基百科所有概念的数目）连接强度</k_j></v_i></li>
<li>T的语义表示向量（semantic interpretation vector）长度为N，T中单词与每个维基百科概念（concepts）的关联强度为$\sum _w<em>i</em>\in _T v_i \cdot k_j.$</li>
<li>计算两个文本的相似度就可以用余弦相似度进行比较</li>
</ul>
<hr>
<center><img src="./图1.png" alt="Alt text"></center><br><center>图1 Semantic interpreter</center>

<hr>
<center><img src="./表1.png" alt="Alt text"></center><br><center>表1 对于单个词（“设备”和“投资者”）最相关的10个维基百科概念</center>

<hr>
<center><img src="./表2.png" alt="Alt text"></center><br><center>表2 对于段落来说，能够把较为模糊的概念表现出来</center>

<hr>
<p>能够通过考虑在它们的上下文中的模糊词来执行词义解码</p>
<center><img src="./表3.png" alt="Alt text"></center><br><center>表3 包含模糊词“bank”和“jaguar”的短语的向量中的第一个条目</center>

<hr>
<p>###实证评价</p>
<p>####实验过程<br>2006年3月26日，实施ESA方法：<br>解析维基百科XML转储——&gt;删除小的、特定的概念（少于100个词、少于5个传入、传出链接）——&gt;删除停用词、稀有词——&gt;389202个术语，即<strong>N=389202</strong>.<br>使用2004年4月的ODP快照，构建了基于开放目录项目（the Open Directory Project, ODP）的语义解释器：<br>修建非英语材料——&gt;抓取每个站点的前10个页面来扩充训练数据——&gt;20700000个术语，即<strong>N=20700000</strong>.</p>
<p>评估词相关性：<br>WordSimilarity-353 collection 2，包含353个词对，每对有13-16个人类判断，使用斯皮尔曼比较计算得分与人类判断的相关性。<br>评估文档相似性：<br>澳大利亚广播公司的新闻邮件服务的50个文档集合，每1225对有8-12人判断，使用皮尔逊线性相关系数比较计算得分与人类判断的相关性。</p>
<hr>
<p>####实验结果</p>
<center><img src="./表4.png" alt="Alt text"></center><br><center>表4 应用ESA估计单个词的相关性结果</center><br>两种ESA技术比以前的研究有实质性改进，比最近引入的基于维基百科的其他方法有更好的结果。<br><br>——————–<br><center><img src="./表5.png" alt="Alt text"></center><br><center>表5 应用ESA计算整个文档的相关性结果</center>

<hr>
<p>###与相关研究的比较<br>量化文本的语义相关性是计算语言学很多基本任务的基础，包括词义消歧、信息检索、词和文本聚类和误差校正。<br>先前工作追求的三个主要方向：</p>
<ul>
<li>比较在矢量空间中用词袋模型表示的文本片段[Baeza-Yatesand Ribeiro-Neto, 1999]</li>
<li>利用词汇资源（lexical resources）</li>
<li>使用潜在语义分析（LSA）[Deerwester et al., 1990]</li>
</ul>
<blockquote>
<p>第一种技术最简单，但当待比较的文本共享几个词时，如使用同义词传达类似消息，这种技术就不合适了，对于比较单个词语也是不合适的，后两种技术试图规避这种限制；<br>第二种词汇数据库如WordNet[Fellbaum, 1998] 或Roget’s Thesaurus [Roget, 1852]定义了词间关系如同义词、上下位类，使用这些资源的基础图结构的各种属性计算相关性。缺点是需要大量专业知识、时间、精力，结果只涵盖了语言词典的一小部分；有强烈的词汇取向，只含有单个词的信息。它与ESA都处理一组概念，然而，它们之间有几个差异：</p>
<ul>
<li>WordNet只限于单个词比较，如要比较较长文本，需要额外的复杂程度，而ESA可以以相同的方式处理词和文本；</li>
<li>WordNet不能实现消歧，而ESA可以实现；</li>
<li>WordNet中的词映射相当于简单查找，没有权重，每个词互相排斥。而ESA可以分析大量文本，提供更复杂的词语到概念的映射，并将词或文本的含义表示为概念的加权组合，产生文本多方面的表示。</li>
</ul>
<p>第三种LSA是一种纯统计技术，利用来自大量未标记的文本语料库的词共现信息，不依赖于人类组织的知识。而是通过将奇异值分解（SVD）应用于逐个文档生成矩阵来学习和表示，本质上是一种降维技术，它识别数据中的许多最突出的维度，假设它们对应于“潜在概念”。然后，在这些概念所定义的空间中比较词语和文档的含义。所以它是难以解释的，而ESA使用自然概念表示文本片段的意义，容易理解。</p>
</blockquote>
<hr>
<p>####另外要强调的</p>
<ul>
<li>这种计算语义相似性的方法可能会让人联想到分布相似性（distributional similarity[Lee, 1999; Dagan et al,. 1999]）。ESA的本质是通过比较自然语言文档的大集合中的出现模式来比较单词的含义。该自然语言文档集合中的每个文件集中在一个单一的主题。</li>
<li>文本中强调的是语义相关性（semantic relatedness）而不是语义相似性（semantic similarity）或者语义举例（semantic distance）。相关性的概念更为一般，包括更多不同的特定关系，计算语言学应用往往需要相关性的测量，而不是更侠义的相似性测量，而语义举例的概念因为在文献中使用的方式不同可能会令人困惑。</li>
</ul>
<hr>
<p>####语义相似性的研究工作</p>
<ul>
<li>用R&amp;G[Rubenstein和Goodenough，1965]65个单词对的列表和M&amp;C[Miller和Charles，1991]30个单词对的列表。当只考虑相似关系的时候，使用词汇资源更优于ESA模型</li>
<li>Sahami和Heilman [2006]提出使用Web作为测量短文本片段的相似性的知识来源。 这种技术的一个主要限制是它只适用于短文本，因为发送长文本作为查询到搜索引擎很可能返回很少甚至没有结果。 而ESA模型适用于任意长度的文本片段。</li>
<li>Strube和Ponzetto [2006]也使用维基百科计算语义相关性，方法称为WikiRelate ,与ESA完全不同。 给定一对词w1和w2，WikiRelate搜索维基百科文章p1和p2，它们的标题中分别包含w1和w2。 然后使用p1和p2之间的各种距离度量来计算语义相关性。 这些措施要么依赖于页面的文本，要么依赖于维基百科的类别层级内的路径距离。而ESA模型表示每个词作为维基百科概念的加权向量，然后通过比较两个概念向量来计算语义相关性。它们之间的不同点有：<ul>
<li>WikiRelate只能处理实际出现在维基百科文章标题中的字词。 ESA只要求该词出现在维基百科文章的文本中。</li>
<li>WikiRelate只限于单个词，而ESA可以比较任何长度的文本。</li>
<li>WikiRelate通过与其相关联的文章的文本或由类别层次结构中的节点表示单词的语义。 ESA具有基于维基百科概念的加权向量的更复杂的语义表示。</li>
</ul>
</li>
</ul>
<hr>
<p>##Explicit Semantic Path Mining via Wikipedia Knowledge Tree</p>
<p><a href="file:///D:/文档/literature/[2014.01][][Explicit%20Semantic%20Path%20Mining%20via%20Wikipedia%20].pdf" target="_blank" rel="external"><code>文献2</code></a></p>
<p>Author : <strong>Tian Xia</strong>: School of Information Resource Management, Renmin University of China ; <strong>Miao Chen</strong>: Data To Insight Center Indiana University 、 <strong>Xiaozhong Liu</strong>: Department of Information and Library Science, Indiana University Bloomington</p>
<p>ASIST 2014,November </p>
<hr>
<p>###理论介绍</p>
<p>####问题及研究背景</p>
<p>已实现的信息检索/推荐系统有<br>（1）基于词袋模型（假设单词是所给文档的基本的语义单元），然而最近的研究开始质疑词袋模型的方法和TFIDF表示方式，而是识别语料库的知识地图（Liu et al., 2014），将每个文档定位在用于信息检索的异构图上；<br>（2）ESA，生成概念级特征，可提高标准语料库的文本分类性能。但是准确性是有问题的。例如<code>Iraq&#39;s Top Shiite Cleric Calls for New Government</code> 排在前面的维基百科概念（concepts）包括<code>John Flower</code> , <code>Iraqi National List</code> , <code>Hammadi Ahmad</code> , <code>Promised Day Brigades</code> 。可能很难表达出所给文本的语义，甚至他们可能只在统计上是有用的。</p>
<hr>
<p>####研究目的<br>新的方法：显式语义路径挖掘（Explicit Semantic Path Mining, ESPM），利用维基百科丰富的链接和分类关系，识别维基百科分类树上的优化的<strong>语义路径</strong>，而不是语义概念向量。<br>例如，对于之前的输入，可以生成语义路径 <code>Politics -&gt; Politics by country -&gt; Politics of Iraq -&gt; Iraqi nationalism</code>;<br>另一个例子：<br>输入是<code>Construction of world&#39;s biggest optical telescope starts with a bang</code>,ESPM会在维基百科分类树上找到语义路径<code>Science -&gt; Scientific instruments -&gt; Astronomical</code>。<br>使用<strong>ODP数据</strong>评估显示ESPM是一种有效的算法，用于探索给定文本的深度分类知识，对<em>文本挖掘和检索任务</em>可能是重要的。</p>
<hr>
<p>####实现过程<br>维基百科有面向用户的分层类别定义。顶级类别由专业编辑定义，包括26个类，页面作者和其他贡献者定义底层类别。语义路径就是第一级到较低级别的多个链接的连接。</p>
<center><img src="./2图1.png" alt="Alt text"></center><br><center>图1 ESPM过程（相关的维基百科页面为树上的分类路径“投票”）</center>

<hr>
<p>1.给定输入文本，用ESA生成概念向量$V_T={ P(w_1|text), P(w_2|text),……P(w_n|text)}$，P是文本与维基百科概念（concepts）的相关度。<br>2.用$V_T$中n个概念（concepts）为维基百科分类树上的节点的重要性和概念“投票”。有两个前提：</p>
<hr>
<ul>
<li>$V_T$中所有概念都是被页面链接的。那么维基百科类目（category）对文本的重要性（文本属于某一类别）可表示为$P(C_i|text)$，被所有属于这一类别（$C_i$）的所有页面计算，$w_j \in C_j$，并且跟这些页面有链接关系的都属于一个类别，$w_k &lt;-&gt; w_j(w_k, w_j \in C_j)$。$|w_j \in C_i|$是一个类别中维基百科页面的总数。公式可表示为：$$P(C<em>i|text) = \frac {\sum</em>{w_j \in C_i}(\lambda \cdot P(w<em>j|text) + (1- \lambda )\cdot \frac{\sum</em>{w_k&lt;-&gt;w_j} P(w_k|text)}{|w_k&lt;-&gt;w_j|})}{|w_j \in C_i|}$$得到给定文本的类别的概率分布，需要在树状类别图上归一化类别概率$P’(C_i|text)$，因为父节点或子节点可能影响用于识别关键路径的文本相关的概率。首先要对类别概率进行归一化，确保根节点概率总和为1，其次将每个节点的概率迭代地传给父节点，概率将通过所有可能的路径传递到根节点。$$P’(C<em>i|text) = \frac{\sum</em>{C<em>i-&gt;C</em>{child<em>k}}P(C</em>{child_k}|text)}{|C<em>i -&gt;C </em>{child_k}|}$$通过自底向上方法，所有节点被分配值。使用自上而下的方法找到从根节点道种子节点所有可能的路径，将路径权重定义为路径上所有类别节点的总和：$$P(path<em>k|text) = \frac{\sum </em>{C_i \in path_k} P’(C_i|text)}{|C_i \in path_k|}$$</li>
</ul>
<hr>
<ul>
<li>在维基百科上找到独立的路径，例如，如果我们找到AA–&gt;BB–&gt;CC，不想找到AA–&gt;BB–&gt;CC–&gt;DD，但它们很相似，有依赖关系。使用贪婪算法识别树上的前k个独立重要路径。首先，用上述方法计算所有相关路径的权重，生成一个图。对于依赖性测量，用两路径间的相似性，如果$Sim(p_i, p_j) &gt; \Lambda$ ，那么路径Pi和Pj是互相依赖的。首先选出具有最大权重的节点，删除链接到此节点的所有节点（路径）。重复此过程，直到图表上所有节点被删除和挑选。得到路径的排序列表，每个路径都是独立的。贪婪算法在树挖掘和特征选择研究中被证明是有效的[Chakrabarti &amp; Mehta 2010]。</li>
</ul>
<hr>
<p>###实验验证<br>使用ODP数据评估算法性能，目的是</p>
<ol>
<li>测试语义路径排名的准确性，使用BestMatch@k作为评价标准；</li>
<li>用户希望花费多少努力在结果中找到第一个正确的路径，使用MRR(Mean Reciprocal Rank)作为评估度量。</li>
</ol>
<p>ODP的文本片段是输入，用ESPM估计语义路径pi，ODP的目录类别路径cp作为真值。$sim(p_i, cp)$为cp上重叠节点除以cp上节点总数。如果$sim(p_i, cp) &gt; \beta$，则估计的路径是正确的。</p>
<hr>
<p>####实验结果</p>
<ul>
<li>从语义路径的精度来看，ESPM可以针对ODP判断数据生成高质量的路径排名</li>
<li>类别表现不同，艺术、商业、体育表现优于科学、社会类别</li>
<li>用户期望在排名靠前的路径集合中找到更准确的结果</li>
</ul>
<hr>
<p>###结论</p>
<ul>
<li>与ESA不同，考虑了三种关系：维基百科页面之间的输入输出链接、类别概念关系以及维基百科类别之间的层次关系。语义类别路径挖掘对于文本挖掘和检索研究是重要的。</li>
<li>实验评估表明ESPM是找到给定文本的语义分类路径的较好的方法，重要路径可以从有噪声的数据中区分优先级，比其他方法更有信息。</li>
<li>下一步，使用ESPM算法解决文本分类或信息检索问题，使用更复杂的概率模型，即监督主题建模算法[Ramage et al., 2009]表征词类别信息</li>
</ul>
<hr>
<p>##Semantic Annotation with RescoredESA: Rescoring Concept Features Generated From Explicit Semantic Analysis</p>
<p><a href="file:///D:/文档/literature/[2014.11][][Semantic%20Annotation%20with%20RescoredESA%20Rescoring%20Concept%20Features%20Generated%20From%20Explicit%20Semantic%20Analysis].pdf" target="_blank" rel="external"><code>文献3</code></a></p>
<p>Author : <strong>Zhuoren Jiang</strong>: College of Transportation Management Dalian Maritime University; <strong>Miao Chen</strong>: Data to Insight Center Indiana University ; <strong>Xiaozhong Liu</strong>: Department of Information and Library Science, Indiana University Bloomington</p>
<p>ESAIR 14, November 7, 2014</p>
<hr>
<p>###理论介绍</p>
<p>####研究背景<br>大背景：语义注释，它协助文本挖掘和文本分析。<br>提出一种新方法，叫RescoredESA。取代ESA中的特征选择，对维基百科概念特征的重新定义。<br>目的：通过引入附加信息来改进ESA概念向量和性能，使用上下文信息，即出现在文本中的概念以及概念链接结构以丰富ESA向量：</p>
<ul>
<li>将文本中的概念（concept）与ESA结合起来，提高ESA技能</li>
<li>在ESA向量概念和内部概念之间建立连接，通过使用维基百科文章之间的链接相似性量化连接</li>
<li>评估ESA向量用于文本分类的效果，观察是否改变ESA向量可以达到优化的目的</li>
</ul>
<hr>
<p>####实现过程</p>
<ul>
<li>获得ESA维基百科向量$E = [C<em>{E1}, C</em>{E2}. ……, C<em>{En}]$，扫描文档识别文中出现的维基百科概念$I = [C</em>{I1}, C<em>{I2},……, C</em>{Ik}]$，称为文本维基百科概念。</li>
<li>如果维基百科概念出现在文本中或与文本维基百科概念有高相似性，应该在ESA向量中给予更高的权重。</li>
<li>计算ESA概念向量E中每个维基百科概念与具有k个概念维度的文本维基百科向量I中的每个概念的相似性，与权重相加赋给ESA向量。RescoredESA向量$V_R = [r_1, r_2, ……, r_n]$, 而$r_i = \frac{e<em>i+\alpha(\frac{\sum^k</em>{j=1}LS(C<em>{Ei},C</em>{Ij})}{k})}{1+\alpha}$，$LS(C<em>{Ei}, C</em>{Ij})$是E向量与I向量之间的链接相似性，k是向量I中概念的数目。α用于平均链接相似性和原始ESA概念得分的平滑参数。（链接相似性：基于它们包含的传入传出链接计算两个概念之间的相似性，基于四种类型：交集大小、联合大小、归一化链接距离和链接向量相似性）。</li>
</ul>
<hr>
<center><img src="./3图1.jpg" alt="Alt text"></center><br><center>图1 rescoringESA 概念得分的图示</center>

<hr>
<p>###实验与结果</p>
<p>####实验部署<br>数据：20个新闻组数据<br>与RescoredESA比较的方法：</p>
<ul>
<li>词袋模型（BOW）：使用词向量</li>
<li>文本维基百科概念：使用文档中标识的维基百科概念的向量</li>
<li>ESA维基百科概念：原始ESA方法</li>
<li>上述两种方法的结合，例如$BOW+\gamma \cdot RescoredESA$</li>
</ul>
<p>文本分类方法：最大熵（ME）、贝叶斯（NB）、支持向量机（SVM）和BalancedWinnow（BW）。</p>
<p>####实验结果<br>没有参数调整（$\alpha = 1, \gamma = 1$）：</p>
<center><img src="./3表1.png" alt="Alt text"><br></center><br><center>表1 所有年代引文推荐的元路径</center><br>最大熵、贝叶斯、支持向量机的文本分类效果：Concat(BOW+RescoredESA) &gt; BOW &gt; InText &gt; RescoredESA &gt; ESA；<br>对于BalancedWinnow来说：Concat(BOW + RescoredESA) &gt; BOW &gt; InText &gt; ESA &gt; RescoredESA。<br><br>—————–<br>有参数调整：<br>$\alpha$是平均链接相似性和原始ESA分数之间的平滑参数，$\alpha$越大，平均链接相似性对于最终分数来说越重要。<br><center><img src="./3图2.png" alt="Alt text"></center><br><center>图2 不同$\alpha$值RescoredESA值的F测量值</center>

<hr>
<p>有参数调整：<br>$\gamma$调整词袋模型向量和RescoredESA向量在连接向量中的重要性：$BOW+\gamma \cdot RescoredESA$。</p>
<center><img src="./1482377062340.png" alt="Alt text"><br></center><br><center>图3 在不同$\gamma$值下的Concat(BOW+RescoredESA)效果的F测量值</center>

<hr>
<p>###结果与讨论<br>注释文本文档：联合使用BOW和RescoredESA时，效果可以胜过BOW和其他向量类型，但RescoredESA本身效果并不比BOW更好</p>
<hr>
<center>END<br>敬请批评指正</center>


        </div>
      </div>
    </div>
  </div>
</article>



    <!-- Footer -->
<footer>
  <div class="container">
    <div class="row">
      <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
        <p class="copyright text-muted">
          Theme By <a target="_blank" href="https://github.com/levblanc">Levblanc.</a>
          Inspired By <a target="_blank" href="https://github.com/klugjo/hexo-theme-clean-blog">Clean Blog.</a>
        <p class="copyright text-muted">
          Powered By <a target="_blank" href="https://hexo.io/">Hexo.</a>
        </p>
      </div>
    </div>
  </div>
</footer>


    <!-- After Footer Scripts -->
<script src="/js/highlight.pack.js"></script>
<script>
  document.addEventListener("DOMContentLoaded", function(event) {
    var codeBlocks = Array.prototype.slice.call(document.getElementsByTagName('pre'))
    codeBlocks.forEach(function(block, index) {
      hljs.highlightBlock(block);
    });
  });
</script>

  </body>
</html>

